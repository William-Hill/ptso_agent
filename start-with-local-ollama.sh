#!/bin/bash

# Startup script for A2A agents with local Ollama instance

set -e

echo "ğŸ¤– Starting PTSO Agent A2A System with Local Ollama"
echo "====================================================="

# Check if Ollama is running locally
echo "ğŸ” Checking local Ollama instance..."
if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "âœ… Local Ollama is running"
    
    # Show available models
    echo "ğŸ“‹ Available models:"
    curl -s http://localhost:11434/api/tags | jq -r '.models[].name' 2>/dev/null || echo "   (jq not installed, showing raw output)"
    curl -s http://localhost:11434/api/tags 2>/dev/null || echo "   Could not fetch models"
else
    echo "âŒ Local Ollama is not running!"
    echo "   Please start Ollama first:"
    echo "   ollama serve"
    echo ""
    echo "   Then pull a model:"
    echo "   ollama pull llama3.1:8b"
    exit 1
fi

# Check if Docker and Docker Compose are installed
if ! command -v docker &> /dev/null; then
    echo "âŒ Docker is not installed. Please install Docker first."
    exit 1
fi

if ! command -v docker compose &> /dev/null; then
    echo "âŒ Docker Compose is not installed. Please install Docker Compose first."
    exit 1
fi

# Create .env file if it doesn't exist
if [ ! -f .env ]; then
    echo "ğŸ“ Creating .env file for local Ollama..."
    cat > .env << EOF
# LLM Configuration for Local Ollama
OLLAMA_BASE_URL=http://host.docker.internal:11434
LLM_PROVIDER=ollama

# Database
DATABASE_URL=postgresql://ptso_user:ptso_password@postgres:5432/ptso_db

# Agent URLs
WEATHER_AGENT_URL=http://weather-agent:8001
WARDROBE_AGENT_URL=http://wardrobe-agent:8002
EOF
    echo "âœ… Created .env file for local Ollama configuration"
fi

echo ""
echo "ğŸš€ Starting A2A agents with local Ollama..."

# Build and start services
echo "ğŸ”¨ Building Docker images..."
docker compose -f docker-compose-local-ollama.yml build

echo "ğŸš€ Starting services..."
docker compose -f docker-compose-local-ollama.yml up -d

# Wait for services to be ready
echo "â³ Waiting for services to start..."
sleep 15

# Check service health
echo "ğŸ¥ Checking service health..."

# Check database
if docker compose -f docker-compose-local-ollama.yml exec postgres pg_isready -U ptso_user -d ptso_db; then
    echo "âœ… Database is ready"
else
    echo "âŒ Database is not ready"
fi

# Check weather agent
if curl -s http://localhost:8001/health > /dev/null; then
    echo "âœ… Weather Agent is ready"
else
    echo "âŒ Weather Agent is not ready"
fi

# Check wardrobe agent
if curl -s http://localhost:8002/health > /dev/null; then
    echo "âœ… Wardrobe Agent is ready"
else
    echo "âŒ Wardrobe Agent is not ready"
fi

# Check PTSO agent
if curl -s http://localhost:8000/health > /dev/null; then
    echo "âœ… PTSO Agent is ready"
else
    echo "âŒ PTSO Agent is not ready"
fi

echo ""
echo "ğŸ‰ A2A Agent System is running with local Ollama!"
echo ""
echo "ğŸ“Š Service URLs:"
echo "  â€¢ PTSO Agent (Main):     http://localhost:8000"
echo "  â€¢ Weather Agent:         http://localhost:8001"
echo "  â€¢ Wardrobe Agent:        http://localhost:8002"
echo "  â€¢ Database:              localhost:5433"
echo "  â€¢ Local Ollama:          http://localhost:11434"
echo ""
echo "ğŸ” Health Checks:"
echo "  â€¢ PTSO Agent Health:     http://localhost:8000/health"
echo "  â€¢ Weather Agent Health:  http://localhost:8001/health"
echo "  â€¢ Wardrobe Agent Health: http://localhost:8002/health"
echo "  â€¢ Ollama Health:         http://localhost:11434/api/tags"
echo ""
echo "ğŸ’¬ Test the system:"
echo "  curl -X POST http://localhost:8000/ask -H 'Content-Type: application/json' -d '{\"message\": \"What should I wear in Atlanta today?\"}'"
echo ""
echo "ğŸ§ª Run comprehensive tests:"
echo "  python test-a2a-agents.py"
echo ""
echo "ğŸ›‘ To stop the system:"
echo "  docker-compose -f docker-compose-local-ollama.yml down"
echo ""
echo "ğŸ’¡ Tips:"
echo "  â€¢ Your local Ollama models are shared with the Docker containers"
echo "  â€¢ No need to run Ollama in Docker - using your local instance"
echo "  â€¢ Models are cached locally for faster startup"
